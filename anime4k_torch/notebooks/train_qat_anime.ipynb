{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization Aware Training (QAT)\n",
    "\n",
    "References:\n",
    "\n",
    "- [Quantization (PyTorch documentation)](https://pytorch.org/docs/stable/quantization.html)\n",
    "- [`train_quantization.py` (Torchvision reference code)](https://github.com/pytorch/vision/blob/main/references/classification/train_quantization.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from torch import Tensor\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "sys.path.append(os.fspath(Path.cwd().parent.resolve()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anime4k_torch.data import NpyDataset, transform_image_train, transform_image_eval\n",
    "from anime4k_torch.metric import yuv_error\n",
    "from anime4k_torch.model import SR2Model\n",
    "from anime4k_torch.training import eval_loop, train_loop\n",
    "from anime4k_torch.visualization import show_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"../../data/anime-xuanghdu\").resolve()\n",
    "dataloader_config = {\"batch_size\": 32, \"num_workers\": 4, \"pin_memory\": True}\n",
    "\n",
    "resize = (720, 1280)\n",
    "crop_size = 720\n",
    "\n",
    "\n",
    "def transform_train(image) -> Tensor:\n",
    "    return transform_image_train(image, resize=resize, random_crop_size=crop_size)\n",
    "\n",
    "\n",
    "def transform_validation(image) -> Tensor:\n",
    "    return transform_image_eval(image, resize=resize, center_crop_size=crop_size)\n",
    "\n",
    "\n",
    "train_set = NpyDataset(data_dir / \"train.npy\", transform_train)\n",
    "train_loader = DataLoader(train_set, shuffle=True, drop_last=True, **dataloader_config)\n",
    "\n",
    "validation_set = NpyDataset(data_dir / \"validation.npy\", transform_validation)\n",
    "validation_loader = DataLoader(validation_set, **dataloader_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = Path(\"../checkpoints\").resolve()\n",
    "checkpoint_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the \"onednn\" backend because the default \"x86\" does not seem to use the full 8 bits ([reference](https://pytorch.org/docs/stable/quantization.html#best-practices)).\n",
    "\n",
    "Base learning rates: 1e4, 1e-3, 1e-4\n",
    "\n",
    "| Learning Rates | Validation Losses |   |   |\n",
    "| -------------- | ----------------- | - | - |\n",
    "| x1  | 0.00200 | 0.00179 | 0.00147 |\n",
    "| x4  | 0.00172 | 0.00127 | 0.00122 |\n",
    "| x8  | 0.00144 | 0.00118 | 0.00115 |\n",
    "| x10 | 0.00143 | 0.00119 | 0.00115 |\n",
    "| x16 | 0.00135 | 0.00120 | 0.00114 |\n",
    "| x20 | 0.00132 | 0.00119 | 0.00114 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qconfig = torch.ao.quantization.get_default_qat_qconfig(\"onednn\")\n",
    "model = SR2Model(highway_depth=4, block_depth=7, qconfig=qconfig)\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "# elif torch.backends.mps.is_available():\n",
    "#     device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# TODO: Fuse conv and relu operations.\n",
    "\n",
    "model = torch.ao.quantization.prepare_qat(model)\n",
    "train_losses = []\n",
    "\n",
    "# Super-convergence with clipping\n",
    "optimizer = SGD(model.parameters(), lr=1e5, momentum=0.9, nesterov=True)\n",
    "train_losses.extend(\n",
    "    train_loop(model, optimizer, yuv_error, train_loader, epochs=1, clip_grad_norm=1e-8)\n",
    ")\n",
    "validation_loss = eval_loop(model, yuv_error, validation_loader)\n",
    "print(\"Validation loss:\", validation_loss)\n",
    "\n",
    "# Fine tuning with Adam\n",
    "clip_grad_norm = None\n",
    "for learning_rate in 1e-2, 1e-3:\n",
    "    optimizer = Adam(model.parameters(), learning_rate)\n",
    "    train_losses.extend(\n",
    "        train_loop(\n",
    "            model,\n",
    "            optimizer,\n",
    "            yuv_error,\n",
    "            train_loader,\n",
    "            epochs=1,\n",
    "            clip_grad_norm=clip_grad_norm,\n",
    "        )\n",
    "    )\n",
    "    validation_loss = eval_loop(model, yuv_error, validation_loader)\n",
    "    print(\"Validation loss:\", validation_loss)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    model.cpu()\n",
    "    model.eval()\n",
    "    model = torch.ao.quantization.convert(model)\n",
    "    torch.save(model.state_dict(), checkpoint_dir / \"anime_i8.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses)\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Losses\")\n",
    "None  # Suppress output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize = (20, 20)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    inputs, targets = next(iter(DataLoader(train_set, batch_size=5)))\n",
    "    model.eval()\n",
    "    outputs = model(inputs)\n",
    "\n",
    "figure = plt.figure(figsize=figsize)\n",
    "show_images(figure, inputs)\n",
    "figure.axes[0].set_title(\"Inputs\")\n",
    "\n",
    "figure = plt.figure(figsize=figsize)\n",
    "show_images(figure, targets)\n",
    "figure.axes[0].set_title(\"Targets\")\n",
    "\n",
    "figure = plt.figure(figsize=figsize)\n",
    "show_images(figure, outputs)\n",
    "figure.axes[0].set_title(\"Outputs\")\n",
    "\n",
    "upsampled_inputs = F.interpolate(inputs, scale_factor=2, mode=\"bilinear\")\n",
    "\n",
    "figure = plt.figure(figsize=figsize)\n",
    "show_images(figure, targets - upsampled_inputs, value_range=(-0.2, 0.2))\n",
    "figure.axes[0].set_title(\"Targets - Inputs\")\n",
    "\n",
    "figure = plt.figure(figsize=figsize)\n",
    "show_images(figure, outputs - upsampled_inputs, value_range=(-0.2, 0.2))\n",
    "figure.axes[0].set_title(\"Outputs - Inputs\")\n",
    "\n",
    "None  # Suppress output."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anime4k_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
