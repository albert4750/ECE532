{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch.ao.quantization.quantize_pt2e import convert_pt2e, prepare_pt2e\n",
    "from torch.ao.quantization.quantizer.x86_inductor_quantizer import (\n",
    "    X86InductorQuantizer,\n",
    "    get_default_x86_inductor_quantization_config,\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "from torch._export import capture_pre_autograd_graph\n",
    "\n",
    "sys.path.append(os.fspath(Path.cwd().parent.resolve()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anime4k_torch.data import NpyDataset, transform_image_eval\n",
    "from anime4k_torch.model import SR2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"../../data/synla\").resolve()\n",
    "validation_set = NpyDataset(data_dir / \"synla_1024.npy\", transform_image_eval)\n",
    "validation_loader = DataLoader(validation_set, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = Path(\"../checkpoints\").resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SR2Model(highway_depth=4, block_depth=7)\n",
    "model.load_state_dict(torch.load(checkpoint_dir / \"synla_f32.pth\"))\n",
    "model.eval()\n",
    "model = capture_pre_autograd_graph(model, (next(iter(validation_loader))[0],))\n",
    "quantizer = X86InductorQuantizer().set_global(\n",
    "    get_default_x86_inductor_quantization_config()\n",
    ")\n",
    "model = prepare_pt2e(model, quantizer)\n",
    "with torch.inference_mode():\n",
    "    for images, _ in validation_loader:\n",
    "        model(images)\n",
    "model = convert_pt2e(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class GraphModule(torch.nn.Module):\n",
      "    def forward(self, x):\n",
      "        arg0: \"f32[1, 3, 128, 128]\"; \n",
      "    \n",
      "        arg0, = fx_pytree.tree_flatten_spec(([x], {}), self._in_spec)\n",
      "        arg0_1 = arg0\n",
      "        \n",
      "        # No stacktrace found for following nodes\n",
      "        quantize_per_tensor_default = torch.ops.quantized_decomposed.quantize_per_tensor.default(arg0_1, 0.003921329043805599, 0, 0, 255, torch.uint8)\n",
      "        \n",
      "        # File: /home/yunhao/ECE532-Digital-Systems-Design/ECE532/anime4k_torch/anime4k_torch/model.py:83, code: x = conv_layer(x)\n",
      "        dequantize_per_tensor_default = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default, 0.003921329043805599, 0, 0, 255, torch.uint8);  quantize_per_tensor_default = None\n",
      "        \n",
      "        # File: /home/yunhao/ECE532-Digital-Systems-Design/ECE532/anime4k_torch/anime4k_torch/model.py:76, code: upsampled_x = F.interpolate(x, scale_factor=2, mode=\"bilinear\")\n",
      "        upsample_bilinear2d: \"f32[1, 3, 256, 256]\" = torch.ops.aten.upsample_bilinear2d.vec(arg0_1, None, False, [2.0, 2.0]);  arg0_1 = None\n",
      "        \n",
      "        # File: /home/yunhao/ECE532-Digital-Systems-Design/ECE532/anime4k_torch/anime4k_torch/model.py:83, code: x = conv_layer(x)\n",
      "        _param_constant0: \"f32[4, 3, 3, 3]\" = self._param_constant0\n",
      "        \n",
      "        # No stacktrace found for following nodes\n",
      "        conv_layers_slice_none_1_none_0__scale_0 = getattr(self, \"conv_layers[slice(None, -1, None)][0]_scale_0\")\n",
      "        conv_layers_slice_none_1_none_0__zero_point_0 = getattr(self, \"conv_layers[slice(None, -1, None)][0]_zero_point_0\")\n",
      "        \n",
      "        # File: /home/yunhao/ECE532-Digital-Systems-Design/ECE532/anime4k_torch/anime4k_torch/model.py:83, code: x = conv_layer(x)\n",
      "        quantize_per_channel_default = torch.ops.quantized_decomposed.quantize_per_channel.default(_param_constant0, conv_layers_slice_none_1_none_0__scale_0, conv_layers_slice_none_1_none_0__zero_point_0, 0, -128, 127, torch.int8);  _param_constant0 = None\n",
      "        dequantize_per_channel_default = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default, conv_layers_slice_none_1_none_0__scale_0, conv_layers_slice_none_1_none_0__zero_point_0, 0, -128, 127, torch.int8);  quantize_per_channel_default = conv_layers_slice_none_1_none_0__scale_0 = conv_layers_slice_none_1_none_0__zero_point_0 = None\n",
      "        _param_constant1: \"f32[4]\" = self._param_constant1\n",
      "        conv2d: \"f32[1, 4, 128, 128]\" = torch.ops.aten.conv2d.default(dequantize_per_tensor_default, dequantize_per_channel_default, _param_constant1, [1, 1], [1, 1]);  dequantize_per_tensor_default = dequantize_per_channel_default = _param_constant1 = None\n",
      "        \n",
      "        # File: /home/yunhao/ECE532-Digital-Systems-Design/ECE532/anime4k_torch/anime4k_torch/model.py:85, code: x = torch_cat([F.relu(x), F.relu(torch_mul_scalar(x, -1))], dim=1)\n",
      "        relu: \"f32[1, 4, 128, 128]\" = torch.ops.aten.relu.default(conv2d)\n",
      "        \n",
      "        # File: /home/yunhao/miniforge3/envs/anime4k_torch/lib/python3.11/site-packages/torch/ao/nn/quantized/modules/functional_modules.py:64, code: r = torch.mul(x, y)\n",
      "        mul: \"f32[1, 4, 128, 128]\" = torch.ops.aten.mul.Tensor(conv2d, -1);  conv2d = None\n",
      "        \n",
      "        # File: /home/yunhao/ECE532-Digital-Systems-Design/ECE532/anime4k_torch/anime4k_torch/model.py:85, code: x = torch_cat([F.relu(x), F.relu(torch_mul_scalar(x, -1))], dim=1)\n",
      "        relu_1: \"f32[1, 4, 128, 128]\" = torch.ops.aten.relu.default(mul);  mul = None\n",
      "        \n",
      "        # File: /home/yunhao/miniforge3/envs/anime4k_torch/lib/python3.11/site-packages/torch/ao/nn/quantized/modules/functional_modules.py:71, code: r = torch.cat(x, dim=dim)\n",
      "        cat: \"f32[1, 8, 128, 128]\" = torch.ops.aten.cat.default([relu, relu_1], 1);  relu = relu_1 = None\n",
      "        \n",
      "        # No stacktrace found for following nodes\n",
      "        quantize_per_tensor_default_1 = torch.ops.quantized_decomposed.quantize_per_tensor.default(cat, 0.005750608164817095, 0, 0, 255, torch.uint8)\n",
      "        \n",
      "        # File: /home/yunhao/ECE532-Digital-Systems-Design/ECE532/anime4k_torch/anime4k_torch/model.py:83, code: x = conv_layer(x)\n",
      "        dequantize_per_tensor_default_1 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_1, 0.005750608164817095, 0, 0, 255, torch.uint8);  quantize_per_tensor_default_1 = None\n",
      "        _param_constant2: \"f32[4, 8, 3, 3]\" = self._param_constant2\n",
      "        \n",
      "        # No stacktrace found for following nodes\n",
      "        conv_layers_slice_none_1_none_1__scale_0 = getattr(self, \"conv_layers[slice(None, -1, None)][1]_scale_0\")\n",
      "        conv_layers_slice_none_1_none_1__zero_point_0 = getattr(self, \"conv_layers[slice(None, -1, None)][1]_zero_point_0\")\n",
      "        \n",
      "        # File: /home/yunhao/ECE532-Digital-Systems-Design/ECE532/anime4k_torch/anime4k_torch/model.py:83, code: x = conv_layer(x)\n",
      "        quantize_per_channel_default_1 = torch.ops.quantized_decomposed.quantize_per_channel.default(_param_constant2, conv_layers_slice_none_1_none_1__scale_0, conv_layers_slice_none_1_none_1__zero_point_0, 0, -128, 127, torch.int8);  _param_constant2 = None\n",
      "        dequantize_per_channel_default_1 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_1, conv_layers_slice_none_1_none_1__scale_0, conv_layers_slice_none_1_none_1__zero_point_0, 0, -128, 127, torch.int8);  quantize_per_channel_default_1 = conv_layers_slice_none_1_none_1__scale_0 = conv_layers_slice_none_1_none_1__zero_point_0 = None\n",
      "        _param_constant3: \"f32[4]\" = self._param_constant3\n",
      "        conv2d_1: \"f32[1, 4, 128, 128]\" = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_1, dequantize_per_channel_default_1, _param_constant3, [1, 1], [1, 1]);  dequantize_per_tensor_default_1 = dequantize_per_channel_default_1 = _param_constant3 = None\n",
      "        \n",
      "        # File: /home/yunhao/ECE532-Digital-Systems-Design/ECE532/anime4k_torch/anime4k_torch/model.py:85, code: x = torch_cat([F.relu(x), F.relu(torch_mul_scalar(x, -1))], dim=1)\n",
      "        relu_2: \"f32[1, 4, 128, 128]\" = torch.ops.aten.relu.default(conv2d_1)\n",
      "        \n",
      "        # File: /home/yunhao/miniforge3/envs/anime4k_torch/lib/python3.11/site-packages/torch/ao/nn/quantized/modules/functional_modules.py:64, code: r = torch.mul(x, y)\n",
      "        mul_1: \"f32[1, 4, 128, 128]\" = torch.ops.aten.mul.Tensor(conv2d_1, -1);  conv2d_1 = None\n",
      "        \n",
      "        # File: /home/yunhao/ECE532-Digital-Systems-Design/ECE532/anime4k_torch/anime4k_torch/model.py:85, code: x = torch_cat([F.relu(x), F.relu(torch_mul_scalar(x, -1))], dim=1)\n",
      "        relu_3: \"f32[1, 4, 128, 128]\" = torch.ops.aten.relu.default(mul_1);  mul_1 = None\n",
      "        \n",
      "        # File: /home/yunhao/miniforge3/envs/anime4k_torch/lib/python3.11/site-packages/torch/ao/nn/quantized/modules/functional_modules.py:71, code: r = torch.cat(x, dim=dim)\n",
      "        cat_1: \"f32[1, 8, 128, 128]\" = torch.ops.aten.cat.default([relu_2, relu_3], 1);  relu_2 = relu_3 = None\n",
      "        \n",
      "        # No stacktrace found for following nodes\n",
      "        quantize_per_tensor_default_2 = torch.ops.quantized_decomposed.quantize_per_tensor.default(cat_1, 0.004536733031272888, 0, 0, 255, torch.uint8)\n",
      "        \n",
      "        # File: /home/yunhao/ECE532-Digital-Systems-Design/ECE532/anime4k_torch/anime4k_torch/model.py:83, code: x = conv_layer(x)\n",
      "        dequantize_per_tensor_default_2 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_2, 0.004536733031272888, 0, 0, 255, torch.uint8);  quantize_per_tensor_default_2 = None\n",
      "        _param_constant4: \"f32[4, 8, 3, 3]\" = self._param_constant4\n",
      "        \n",
      "        # No stacktrace found for following nodes\n",
      "        conv_layers_slice_none_1_none_2__scale_0 = getattr(self, \"conv_layers[slice(None, -1, None)][2]_scale_0\")\n",
      "        conv_layers_slice_none_1_none_2__zero_point_0 = getattr(self, \"conv_layers[slice(None, -1, None)][2]_zero_point_0\")\n",
      "        \n",
      "        # File: /home/yunhao/ECE532-Digital-Systems-Design/ECE532/anime4k_torch/anime4k_torch/model.py:83, code: x = conv_layer(x)\n",
      "        quantize_per_channel_default_2 = torch.ops.quantized_decomposed.quantize_per_channel.default(_param_constant4, conv_layers_slice_none_1_none_2__scale_0, conv_layers_slice_none_1_none_2__zero_point_0, 0, -128, 127, torch.int8);  _param_constant4 = None\n",
      "        dequantize_per_channel_default_2 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_2, conv_layers_slice_none_1_none_2__scale_0, conv_layers_slice_none_1_none_2__zero_point_0, 0, -128, 127, torch.int8);  quantize_per_channel_default_2 = conv_layers_slice_none_1_none_2__scale_0 = conv_layers_slice_none_1_none_2__zero_point_0 = None\n",
      "        _param_constant5: \"f32[4]\" = self._param_constant5\n",
      "        conv2d_2: \"f32[1, 4, 128, 128]\" = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_2, dequantize_per_channel_default_2, _param_constant5, [1, 1], [1, 1]);  dequantize_per_tensor_default_2 = dequantize_per_channel_default_2 = _param_constant5 = None\n",
      "        \n",
      "        # File: /home/yunhao/ECE532-Digital-Systems-Design/ECE532/anime4k_torch/anime4k_torch/model.py:85, code: x = torch_cat([F.relu(x), F.relu(torch_mul_scalar(x, -1))], dim=1)\n",
      "        relu_4: \"f32[1, 4, 128, 128]\" = torch.ops.aten.relu.default(conv2d_2)\n",
      "        \n",
      "        # File: /home/yunhao/miniforge3/envs/anime4k_torch/lib/python3.11/site-packages/torch/ao/nn/quantized/modules/functional_modules.py:64, code: r = torch.mul(x, y)\n",
      "        mul_2: \"f32[1, 4, 128, 128]\" = torch.ops.aten.mul.Tensor(conv2d_2, -1);  conv2d_2 = None\n",
      "        \n",
      "        # File: /home/yunhao/ECE532-Digital-Systems-Design/ECE532/anime4k_torch/anime4k_torch/model.py:85, code: x = torch_cat([F.relu(x), F.relu(torch_mul_scalar(x, -1))], dim=1)\n",
      "        relu_5: \"f32[1, 4, 128, 128]\" = torch.ops.aten.relu.default(mul_2);  mul_2 = None\n",
      "        \n",
      "        # File: /home/yunhao/miniforge3/envs/anime4k_torch/lib/python3.11/site-packages/torch/ao/nn/quantized/modules/functional_modules.py:71, code: r = torch.cat(x, dim=dim)\n",
      "        cat_2: \"f32[1, 8, 128, 128]\" = torch.ops.aten.cat.default([relu_4, relu_5], 1);  relu_4 = relu_5 = None\n",
      "        \n",
      "        # No stacktrace found for following nodes\n",
      "        quantize_per_tensor_default_3 = torch.ops.quantized_decomposed.quantize_per_tensor.default(cat_2, 0.006515256129205227, 0, 0, 255, torch.uint8)\n",
      "        \n",
      "        # File: /home/yunhao/ECE532-Digital-Systems-Design/ECE532/anime4k_torch/anime4k_torch/model.py:83, code: x = conv_layer(x)\n",
      "        dequantize_per_tensor_default_3 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_3, 0.006515256129205227, 0, 0, 255, torch.uint8);  quantize_per_tensor_default_3 = None\n",
      "        _param_constant6: \"f32[4, 8, 3, 3]\" = self._param_constant6\n",
      "        \n",
      "        # No stacktrace found for following nodes\n",
      "        conv_layers_slice_none_1_none_3__scale_0 = getattr(self, \"conv_layers[slice(None, -1, None)][3]_scale_0\")\n",
      "        conv_layers_slice_none_1_none_3__zero_point_0 = getattr(self, \"conv_layers[slice(None, -1, None)][3]_zero_point_0\")\n",
      "        \n",
      "        # File: /home/yunhao/ECE532-Digital-Systems-Design/ECE532/anime4k_torch/anime4k_torch/model.py:83, code: x = conv_layer(x)\n",
      "        quantize_per_channel_default_3 = torch.ops.quantized_decomposed.quantize_per_channel.default(_param_constant6, conv_layers_slice_none_1_none_3__scale_0, conv_layers_slice_none_1_none_3__zero_point_0, 0, -128, 127, torch.int8);  _param_constant6 = None\n",
      "        dequantize_per_channel_default_3 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_3, conv_layers_slice_none_1_none_3__scale_0, conv_layers_slice_none_1_none_3__zero_point_0, 0, -128, 127, torch.int8);  quantize_per_channel_default_3 = conv_layers_slice_none_1_none_3__scale_0 = conv_layers_slice_none_1_none_3__zero_point_0 = None\n",
      "        _param_constant7: \"f32[4]\" = self._param_constant7\n",
      "        conv2d_3: \"f32[1, 4, 128, 128]\" = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_3, dequantize_per_channel_default_3, _param_constant7, [1, 1], [1, 1]);  dequantize_per_tensor_default_3 = dequantize_per_channel_default_3 = _param_constant7 = None\n",
      "        \n",
      "        # File: /home/yunhao/ECE532-Digital-Systems-Design/ECE532/anime4k_torch/anime4k_torch/model.py:85, code: x = torch_cat([F.relu(x), F.relu(torch_mul_scalar(x, -1))], dim=1)\n",
      "        relu_6: \"f32[1, 4, 128, 128]\" = torch.ops.aten.relu.default(conv2d_3)\n",
      "        \n",
      "        # File: /home/yunhao/miniforge3/envs/anime4k_torch/lib/python3.11/site-packages/torch/ao/nn/quantized/modules/functional_modules.py:64, code: r = torch.mul(x, y)\n",
      "        mul_3: \"f32[1, 4, 128, 128]\" = torch.ops.aten.mul.Tensor(conv2d_3, -1);  conv2d_3 = None\n",
      "        \n",
      "        # File: /home/yunhao/ECE532-Digital-Systems-Design/ECE532/anime4k_torch/anime4k_torch/model.py:85, code: x = torch_cat([F.relu(x), F.relu(torch_mul_scalar(x, -1))], dim=1)\n",
      "        relu_7: \"f32[1, 4, 128, 128]\" = torch.ops.aten.relu.default(mul_3);  mul_3 = None\n",
      "        \n",
      "        # File: /home/yunhao/miniforge3/envs/anime4k_torch/lib/python3.11/site-packages/torch/ao/nn/quantized/modules/functional_modules.py:71, code: r = torch.cat(x, dim=dim)\n",
      "        cat_3: \"f32[1, 8, 128, 128]\" = torch.ops.aten.cat.default([relu_6, relu_7], 1);  relu_6 = relu_7 = None\n",
      "        \n",
      "        # No stacktrace found for following nodes\n",
      "        quantize_per_tensor_default_4 = torch.ops.quantized_decomposed.quantize_per_tensor.default(cat_3, 0.006228369660675526, 0, 0, 255, torch.uint8)\n",
      "        \n",
      "        # File: /home/yunhao/ECE532-Digital-Systems-Design/ECE532/anime4k_torch/anime4k_torch/model.py:83, code: x = conv_layer(x)\n",
      "        dequantize_per_tensor_default_4 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_4, 0.006228369660675526, 0, 0, 255, torch.uint8);  quantize_per_tensor_default_4 = None\n",
      "        _param_constant8: \"f32[4, 8, 3, 3]\" = self._param_constant8\n",
      "        \n",
      "        # No stacktrace found for following nodes\n",
      "        conv_layers_slice_none_1_none_4__scale_0 = getattr(self, \"conv_layers[slice(None, -1, None)][4]_scale_0\")\n",
      "        conv_layers_slice_none_1_none_4__zero_point_0 = getattr(self, \"conv_layers[slice(None, -1, None)][4]_zero_point_0\")\n",
      "        \n",
      "        # File: /home/yunhao/ECE532-Digital-Systems-Design/ECE532/anime4k_torch/anime4k_torch/model.py:83, code: x = conv_layer(x)\n",
      "        quantize_per_channel_default_4 = torch.ops.quantized_decomposed.quantize_per_channel.default(_param_constant8, conv_layers_slice_none_1_none_4__scale_0, conv_layers_slice_none_1_none_4__zero_point_0, 0, -128, 127, torch.int8);  _param_constant8 = None\n",
      "        dequantize_per_channel_default_4 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_4, conv_layers_slice_none_1_none_4__scale_0, conv_layers_slice_none_1_none_4__zero_point_0, 0, -128, 127, torch.int8);  quantize_per_channel_default_4 = conv_layers_slice_none_1_none_4__scale_0 = conv_layers_slice_none_1_none_4__zero_point_0 = None\n",
      "        _param_constant9: \"f32[4]\" = self._param_constant9\n",
      "        conv2d_4: \"f32[1, 4, 128, 128]\" = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_4, dequantize_per_channel_default_4, _param_constant9, [1, 1], [1, 1]);  dequantize_per_tensor_default_4 = dequantize_per_channel_default_4 = _param_constant9 = None\n",
      "        \n",
      "        # File: /home/yunhao/ECE532-Digital-Systems-Design/ECE532/anime4k_torch/anime4k_torch/model.py:85, code: x = torch_cat([F.relu(x), F.relu(torch_mul_scalar(x, -1))], dim=1)\n",
      "        relu_8: \"f32[1, 4, 128, 128]\" = torch.ops.aten.relu.default(conv2d_4)\n",
      "        \n",
      "        # File: /home/yunhao/miniforge3/envs/anime4k_torch/lib/python3.11/site-packages/torch/ao/nn/quantized/modules/functional_modules.py:64, code: r = torch.mul(x, y)\n",
      "        mul_4: \"f32[1, 4, 128, 128]\" = torch.ops.aten.mul.Tensor(conv2d_4, -1);  conv2d_4 = None\n",
      "        \n",
      "        # File: /home/yunhao/ECE532-Digital-Systems-Design/ECE532/anime4k_torch/anime4k_torch/model.py:85, code: x = torch_cat([F.relu(x), F.relu(torch_mul_scalar(x, -1))], dim=1)\n",
      "        relu_9: \"f32[1, 4, 128, 128]\" = torch.ops.aten.relu.default(mul_4);  mul_4 = None\n",
      "        \n",
      "        # File: /home/yunhao/miniforge3/envs/anime4k_torch/lib/python3.11/site-packages/torch/ao/nn/quantized/modules/functional_modules.py:71, code: r = torch.cat(x, dim=dim)\n",
      "        cat_4: \"f32[1, 8, 128, 128]\" = torch.ops.aten.cat.default([relu_8, relu_9], 1);  relu_8 = relu_9 = None\n",
      "        \n",
      "        # No stacktrace found for following nodes\n",
      "        quantize_per_tensor_default_5 = torch.ops.quantized_decomposed.quantize_per_tensor.default(cat_4, 0.005801366176456213, 0, 0, 255, torch.uint8)\n",
      "        \n",
      "        # File: /home/yunhao/ECE532-Digital-Systems-Design/ECE532/anime4k_torch/anime4k_torch/model.py:83, code: x = conv_layer(x)\n",
      "        dequantize_per_tensor_default_5 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_5, 0.005801366176456213, 0, 0, 255, torch.uint8);  quantize_per_tensor_default_5 = None\n",
      "        _param_constant10: \"f32[4, 8, 3, 3]\" = self._param_constant10\n",
      "        \n",
      "        # No stacktrace found for following nodes\n",
      "        conv_layers_slice_none_1_none_5__scale_0 = getattr(self, \"conv_layers[slice(None, -1, None)][5]_scale_0\")\n",
      "        conv_layers_slice_none_1_none_5__zero_point_0 = getattr(self, \"conv_layers[slice(None, -1, None)][5]_zero_point_0\")\n",
      "        \n",
      "        # File: /home/yunhao/ECE532-Digital-Systems-Design/ECE532/anime4k_torch/anime4k_torch/model.py:83, code: x = conv_layer(x)\n",
      "        quantize_per_channel_default_5 = torch.ops.quantized_decomposed.quantize_per_channel.default(_param_constant10, conv_layers_slice_none_1_none_5__scale_0, conv_layers_slice_none_1_none_5__zero_point_0, 0, -128, 127, torch.int8);  _param_constant10 = None\n",
      "        dequantize_per_channel_default_5 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_5, conv_layers_slice_none_1_none_5__scale_0, conv_layers_slice_none_1_none_5__zero_point_0, 0, -128, 127, torch.int8);  quantize_per_channel_default_5 = conv_layers_slice_none_1_none_5__scale_0 = conv_layers_slice_none_1_none_5__zero_point_0 = None\n",
      "        _param_constant11: \"f32[4]\" = self._param_constant11\n",
      "        conv2d_5: \"f32[1, 4, 128, 128]\" = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_5, dequantize_per_channel_default_5, _param_constant11, [1, 1], [1, 1]);  dequantize_per_tensor_default_5 = dequantize_per_channel_default_5 = _param_constant11 = None\n",
      "        \n",
      "        # File: /home/yunhao/ECE532-Digital-Systems-Design/ECE532/anime4k_torch/anime4k_torch/model.py:85, code: x = torch_cat([F.relu(x), F.relu(torch_mul_scalar(x, -1))], dim=1)\n",
      "        relu_10: \"f32[1, 4, 128, 128]\" = torch.ops.aten.relu.default(conv2d_5)\n",
      "        \n",
      "        # File: /home/yunhao/miniforge3/envs/anime4k_torch/lib/python3.11/site-packages/torch/ao/nn/quantized/modules/functional_modules.py:64, code: r = torch.mul(x, y)\n",
      "        mul_5: \"f32[1, 4, 128, 128]\" = torch.ops.aten.mul.Tensor(conv2d_5, -1);  conv2d_5 = None\n",
      "        \n",
      "        # File: /home/yunhao/ECE532-Digital-Systems-Design/ECE532/anime4k_torch/anime4k_torch/model.py:85, code: x = torch_cat([F.relu(x), F.relu(torch_mul_scalar(x, -1))], dim=1)\n",
      "        relu_11: \"f32[1, 4, 128, 128]\" = torch.ops.aten.relu.default(mul_5);  mul_5 = None\n",
      "        \n",
      "        # File: /home/yunhao/miniforge3/envs/anime4k_torch/lib/python3.11/site-packages/torch/ao/nn/quantized/modules/functional_modules.py:71, code: r = torch.cat(x, dim=dim)\n",
      "        cat_5: \"f32[1, 8, 128, 128]\" = torch.ops.aten.cat.default([relu_10, relu_11], 1);  relu_10 = relu_11 = None\n",
      "        \n",
      "        # No stacktrace found for following nodes\n",
      "        quantize_per_tensor_default_6 = torch.ops.quantized_decomposed.quantize_per_tensor.default(cat_5, 0.005822507664561272, 0, 0, 255, torch.uint8)\n",
      "        \n",
      "        # File: /home/yunhao/ECE532-Digital-Systems-Design/ECE532/anime4k_torch/anime4k_torch/model.py:83, code: x = conv_layer(x)\n",
      "        dequantize_per_tensor_default_6 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_6, 0.005822507664561272, 0, 0, 255, torch.uint8);  quantize_per_tensor_default_6 = None\n",
      "        _param_constant12: \"f32[4, 8, 3, 3]\" = self._param_constant12\n",
      "        \n",
      "        # No stacktrace found for following nodes\n",
      "        conv_layers_slice_none_1_none_6__scale_0 = getattr(self, \"conv_layers[slice(None, -1, None)][6]_scale_0\")\n",
      "        conv_layers_slice_none_1_none_6__zero_point_0 = getattr(self, \"conv_layers[slice(None, -1, None)][6]_zero_point_0\")\n",
      "        \n",
      "        # File: /home/yunhao/ECE532-Digital-Systems-Design/ECE532/anime4k_torch/anime4k_torch/model.py:83, code: x = conv_layer(x)\n",
      "        quantize_per_channel_default_6 = torch.ops.quantized_decomposed.quantize_per_channel.default(_param_constant12, conv_layers_slice_none_1_none_6__scale_0, conv_layers_slice_none_1_none_6__zero_point_0, 0, -128, 127, torch.int8);  _param_constant12 = None\n",
      "        dequantize_per_channel_default_6 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_6, conv_layers_slice_none_1_none_6__scale_0, conv_layers_slice_none_1_none_6__zero_point_0, 0, -128, 127, torch.int8);  quantize_per_channel_default_6 = conv_layers_slice_none_1_none_6__scale_0 = conv_layers_slice_none_1_none_6__zero_point_0 = None\n",
      "        _param_constant13: \"f32[4]\" = self._param_constant13\n",
      "        conv2d_6: \"f32[1, 4, 128, 128]\" = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_6, dequantize_per_channel_default_6, _param_constant13, [1, 1], [1, 1]);  dequantize_per_tensor_default_6 = dequantize_per_channel_default_6 = _param_constant13 = None\n",
      "        \n",
      "        # File: /home/yunhao/ECE532-Digital-Systems-Design/ECE532/anime4k_torch/anime4k_torch/model.py:85, code: x = torch_cat([F.relu(x), F.relu(torch_mul_scalar(x, -1))], dim=1)\n",
      "        relu_12: \"f32[1, 4, 128, 128]\" = torch.ops.aten.relu.default(conv2d_6)\n",
      "        \n",
      "        # File: /home/yunhao/miniforge3/envs/anime4k_torch/lib/python3.11/site-packages/torch/ao/nn/quantized/modules/functional_modules.py:64, code: r = torch.mul(x, y)\n",
      "        mul_6: \"f32[1, 4, 128, 128]\" = torch.ops.aten.mul.Tensor(conv2d_6, -1);  conv2d_6 = None\n",
      "        \n",
      "        # File: /home/yunhao/ECE532-Digital-Systems-Design/ECE532/anime4k_torch/anime4k_torch/model.py:85, code: x = torch_cat([F.relu(x), F.relu(torch_mul_scalar(x, -1))], dim=1)\n",
      "        relu_13: \"f32[1, 4, 128, 128]\" = torch.ops.aten.relu.default(mul_6);  mul_6 = None\n",
      "        \n",
      "        # File: /home/yunhao/miniforge3/envs/anime4k_torch/lib/python3.11/site-packages/torch/ao/nn/quantized/modules/functional_modules.py:71, code: r = torch.cat(x, dim=dim)\n",
      "        cat_6: \"f32[1, 8, 128, 128]\" = torch.ops.aten.cat.default([relu_12, relu_13], 1);  relu_12 = relu_13 = None\n",
      "        \n",
      "        # File: /home/yunhao/miniforge3/envs/anime4k_torch/lib/python3.11/site-packages/torch/ao/nn/quantized/modules/functional_modules.py:71, code: r = torch.cat(x, dim=dim)\n",
      "        cat_7: \"f32[1, 56, 128, 128]\" = torch.ops.aten.cat.default([cat, cat_1, cat_2, cat_3, cat_4, cat_5, cat_6], 1);  cat = cat_1 = cat_2 = cat_3 = cat_4 = cat_5 = cat_6 = None\n",
      "        \n",
      "        # No stacktrace found for following nodes\n",
      "        quantize_per_tensor_default_7 = torch.ops.quantized_decomposed.quantize_per_tensor.default(cat_7, 0.00692765461280942, 0, 0, 255, torch.uint8);  cat_7 = None\n",
      "        \n",
      "        # File: /home/yunhao/ECE532-Digital-Systems-Design/ECE532/anime4k_torch/anime4k_torch/model.py:88, code: x = self.conv_layers[-1](x)\n",
      "        dequantize_per_tensor_default_7 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_7, 0.00692765461280942, 0, 0, 255, torch.uint8);  quantize_per_tensor_default_7 = None\n",
      "        _param_constant14: \"f32[12, 56, 1, 1]\" = self._param_constant14\n",
      "        \n",
      "        # No stacktrace found for following nodes\n",
      "        conv_layers_1__scale_0 = getattr(self, \"conv_layers[-1]_scale_0\")\n",
      "        conv_layers_1__zero_point_0 = getattr(self, \"conv_layers[-1]_zero_point_0\")\n",
      "        \n",
      "        # File: /home/yunhao/ECE532-Digital-Systems-Design/ECE532/anime4k_torch/anime4k_torch/model.py:88, code: x = self.conv_layers[-1](x)\n",
      "        quantize_per_channel_default_7 = torch.ops.quantized_decomposed.quantize_per_channel.default(_param_constant14, conv_layers_1__scale_0, conv_layers_1__zero_point_0, 0, -128, 127, torch.int8);  _param_constant14 = None\n",
      "        dequantize_per_channel_default_7 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_7, conv_layers_1__scale_0, conv_layers_1__zero_point_0, 0, -128, 127, torch.int8);  quantize_per_channel_default_7 = conv_layers_1__scale_0 = conv_layers_1__zero_point_0 = None\n",
      "        _param_constant15: \"f32[12]\" = self._param_constant15\n",
      "        conv2d_7: \"f32[1, 12, 128, 128]\" = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_7, dequantize_per_channel_default_7, _param_constant15);  dequantize_per_tensor_default_7 = dequantize_per_channel_default_7 = _param_constant15 = None\n",
      "        \n",
      "        # File: /home/yunhao/ECE532-Digital-Systems-Design/ECE532/anime4k_torch/anime4k_torch/model.py:93, code: x = F.pixel_shuffle(x, upscale_factor=2)  # pylint: disable=not-callable\n",
      "        pixel_shuffle: \"f32[1, 3, 256, 256]\" = torch.ops.aten.pixel_shuffle.default(conv2d_7, 2);  conv2d_7 = None\n",
      "        \n",
      "        # File: /home/yunhao/ECE532-Digital-Systems-Design/ECE532/anime4k_torch/anime4k_torch/model.py:94, code: output = x + upsampled_x\n",
      "        add: \"f32[1, 3, 256, 256]\" = torch.ops.aten.add.Tensor(pixel_shuffle, upsample_bilinear2d);  pixel_shuffle = upsample_bilinear2d = None\n",
      "        return pytree.tree_unflatten([add], self._out_spec)\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "model.print_readable()\n",
    "None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anime4k_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
